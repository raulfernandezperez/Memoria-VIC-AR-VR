%---------------------------------------------------------------------
%
%                          capítulo 5
%
%---------------------------------------------------------------------
\chapter{Desarrollo del Proyecto}
\label{cap5:sec:Desarrollo del Proyecto}

Una vez elegida la tecnología y las plataformas que se van a utilizar se empieza a preparar el desarrollo del proyecto. En este capítulo se explicará que libreras y paquete de \texttt{Unity} que se necesitan para tener un entorno de desarrollo potente e innovador. Además, se explicará cuales han sido las metodologías utilizadas para plasmar el proceso seguido para realizar el \textit{Mocap}. El tipo de información guardada, como se ha realizado la grabación de los movimientos captados por los sensores de realidad virtual y cual ha sido la elección para mostrar dicha información.

En la siguiente sección se especificará el proceso realizado para grabar los movimientos capturados, de modo que el profesor obtendrá una visualización directa de cómo ha quedado grabado el movimiento deseado, sin la necesidad de quitarse las gafas de realidad virtual.

Para ofrecer una aplicación dinámica y que el alumno reciba un \textit{feedback} gestual de los movimientos, se ilustra como se diseña e implementan avatares con animaciones en \texttt{Unity}.

De forma sucesiva, se describe el diseño de los escenarios 3D para ubicar el avatar principal de la aplicación (profesor). De este modo se han diseñado dos escenarios, uno para grabar y reproducir los movimientos realizados por el profesor dentro de \texttt{VR} y otro en \texttt{AR} donde el alumno podrá visualizar las veces necesarias los movimientos realizado por el profesor.

Posteriormente se elaboran una serie de escenas en \texttt{Unity} que servirán para ilustrar la interfaz para cada una de las aplicaciones (VR y AR). Se ha creado un entorno diferenciado según las necesidades dadas por las diferentes plataformas a desarrollar en el proyecto. Entre estos escenarios las aplicaciones cuentan con las siguientes escenas:

\begin{itemize}
	\item \texttt{VR}: Escena principal de VR, escena de grabación de movimientos, escena de reproducción de los movimientos guardados.
 	\item \texttt{AR}: Escena principal de AR (donde el alumno seleccionará el nivel de aprendizaje) y la escena para realizar el entrenamiento.
\end{itemize}

\section{Investigación para el desarrollo de \textit{Mocap} en Realidad Virtual y Realidad Aumentada}

Parar desarrollar cualquier aplicación en \texttt{Unity} es necesario conocer el contenido que te puede ofrecer la tienda de este \textit{Game Engine}. El \texttt{Asset Store de Unity} es el hogar de una creciente biblioteca de \textit{assets} comerciales y gratuitos creados por \texttt{Unity Technologies} y miembros de la comunidad. Hay una gran cantidad de \textit{assets} disponibles, desde texturas, modelos y animaciones hasta ejemplos de proyectos completos, tutoriales y extensiones del editor. Estos \textit{assets} son accesibles desde una interfaz simple dentro de \texttt{Unity} y son descargados e importados directamente en el proyecto creado.

\subsection{SteamVR}
\label{cap5:sec:SteamVR}

El primer paquete necesario para poder desarrollar una aplicación de VR en HTC Vice es SteamVR\footnote{\url{https://www.steamvr.com/es/}}. Con este \textit{Asset}(nombre que se le da a los paquetes de Unity) los desarrolladores podemos dirigir una API a la que se pueden conectar todos los auriculares populares de RV para PC. El moderno \texttt{SteamVR Unity Plugin} gestiona tres cosas principales para los desarrolladores: la carga de modelos 3D para los controladores de RV, el manejo de la entrada de esos controladores y la estimación del aspecto de la mano mientras se utilizan esos controladores. Además de gestionar estas cosas, tenemos un ejemplo de Sistema de Interacción para ayudar a poner en marcha una aplicación de VR. Proporcionando ejemplos concretos de interacción con el mundo virtual y las diferentes APIs. 

Además, nos permite acceder a los juegos a través de una interfaz que se proyecta en una habitación de nuestra realidad virtual, que podemos aprovechar para jugar a cualquier juego que tengamos en Steam VR.

Este paquete tiene todo lo necesario para reconocer la conectividad de los dispositivos utilizados en VR y para poder utilizar los datos obtenidos de los sensores colocados en la habitación de juego. 

El Asset nos ofrece una serie de ejemplos básicos para poder comprender mejor el funcionamiento de VR. A su vez, es necesaria la instalación de \texttt{Steam}\footnote{\url{ https://store.steampowered.com}} ya que trae incorporados los drivers imprescindibles para la correcta configuración de los dispositivos utilizados en VR(controles y \textit{trackers}).

Los ejemplos que tiene implementado el paquete de \texttt{SteamVR Plugin} son variados y pueden servir como un buen punto de partida para desarrollar el proyecto. La escena de ejemplo \textit{Interactions\_Example} incluye todos los componentes principales y es un buen lugar para familiarizarse con el sistema. La escena contiene los siguientes elementos:


\begin{itemize}
	\item \texttt{Player}: este \textit{prefab} es el núcleo de todo el sistema. La mayoría de los demás componentes dependen del jugador para estar presente en la escena.
    \item \texttt{Teleporting}: el \textit{prefab} \textit{Teleporting} maneja toda la lógica de teletransportación del sistema.
    \item \texttt{InteractableExample}: muestra un interacción simple sobre los aspectos básicos para recibir mensajes de las manos y cómo reaccionar ante estas notificaciones.
    \item \texttt{Throwables}: muestra cómo se puede utilizar el sistema para interactuar con los objetos y crear diferentes tipos de objetos tirables.
    \item \texttt{Skeleton}: diferentes objetos de modelos de mano junto con opciones para determinar a qué mano corresponden del esqueleto.
    \item \texttt{Proximity Button}: una tarea común es presionar un botón. Los botones físicos son más satisfactorios de usar que las interfaces planas, pero los sistemas de interacción física pueden volverse complejos rápidamente. Se ha incluido un botón que se puede presionar con solo estar cerca de un controlador.
    \item \texttt{Interesting Interactables}: Estos son ejemplos un poco más complejos del uso del sistema \texttt{Skeleton Poser} junto con \texttt{Throwables}. Dependiendo del objeto con el que interactuar obtienes diferentes poses de acción.
    \item \texttt{UI}: muestra cómo se manejan las sugerencias en el sistema de interacción y cómo se puede usar para interactuar con los \textit{widgets} de la interfaz, como botones.
    \item \texttt{LinearDrive}: este es un objeto un poco más complejo que combina algunas piezas diferentes para crear un objeto animado que se puede controlar mediante interacciones simples.
    \item \texttt{CircularDrive}: esto muestra cómo las interacciones se pueden restringir y mapear de manera diferente para realizar movimientos más complejos.
    \item \texttt{Longbow}: es uno de los objetos más complejos en el \textit{Asset} y muestra cómo se pueden combinar piezas simples para crear una mecánica de juego completa. 
\end{itemize}

Repasando los diferentes objetos en esta escena de ejemplo te das una amplia idea de la amplitud del sistema de interacción y cómo combinar sus diferentes partes para crear acciones complejas.

Una vez explicado los ejemplos, se elige cuál de ellos se podría utilizar para aprovechar su funcionalidad y tener un apoyo base para el desarrollo del proyecto. Los ejemplos seleccionados serán el \texttt {Player, Skeleton y UI}. Más adelante se explica con más detalle que se utiliza de estos ejemplos.


\subsection{Final IK}

El uso de la realidad virtual presenta muchos desafíos nuevos para el diseño y desarrollo de captura de movimiento, entre ellos el problema de la cinemática inversa (\textit{Inverse Kinematics}). 

La cinemática inversa es la técnica que permite determinar el movimiento de una cadena de articulaciones para lograr que un actuador final se ubique en una posición concreta. El cálculo de la cinemática inversa es un problema complejo que consiste en la resolución de una serie de ecuaciones cuya solución normalmente no es única.\cite{CinematicaInversa}

Este concepto es muy importante para el desarrollo de animaciones en 3D, donde se utiliza para conectar físicamente los personajes del juego en el mundo tridimensional, tales como la sujeción rígida de los pies en un terreno. Una figura animada se modela con un esqueleto de segmentos rígidos conectados con las articulaciones. El problema de cinemática inversa radica en calcular los ángulos de las articulaciones para una pose deseada. A menudo es más fácil para los diseñadores, artistas y animadores definir la configuración espacial de un conjunto sobre las partes móviles, como los brazos y las piernas, en lugar de manipular directamente los ángulos de las articulares. 

Por lo tanto, la cinemática inversa se utiliza en los sistemas \textit{Mocap} para animar las posiciones de las articulaciones. El conjunto de un esqueleto humano se modela como eslabones rígidos conectados por articulaciones que se definen restricciones geométricas. 

Para realizar un movimiento de cualquier extremidad del cuerpo, se requiere el cálculo de los ángulos de las otras articulaciones conectadas con esa parte del cuerpo. Por ejemplo, la cinemática inversa permite mover la mano de un modelo humano 3D con una posición y orientación deseada. Para su correcto funcionamiento es necesario tener un algoritmo que seleccione el ángulo de rotación correcto para cada una de las articulaciones del cuerpo humano, como la muñeca, el codo y los hombros.

Para solventar el problema de la cinemática inversa (véase Figura \ref{fig:IK}) en las articulaciones del cuerpo humano, se estudió una serie de \textit{Assets} capaces de solucionar el problema.

\begin{figure}[h!]
    \centering
    \animategraphics[loop,autoplay,width=0.8\linewidth]{30}{/IK/IK-}{0}{146}
    \caption{Diferencia entre \textit{Forward Kinematics e Inverse Kinematics}}
    \label{fig:IK}  
\end{figure}

La primera elección fue UMotion \footnote{\url{ https://www.soxware.com/umotion/}}, ya que se trata de un editor de animación que ofrece potentes herramientas y flujos de trabajo dentro de Unity. La creación de animaciones en la misma aplicación y situación en la que se van a utilizar simplifica todo el flujo de trabajo acelerando el desarrollo del proyecto. 

El problema surgió cuando se intentan asignar los diferentes componentes de VR dentro del \textit{Asset}, no era posible realizar dicha acción, no admitía VR, por lo que fue descartada esta opción.

Actualmente no hay muchas soluciones \textit{IK} de cuerpo completo disponibles que cumplan con los requisitos muy específicos del desarrollo en realidad virtual.

Además de la precisión y la calidad general de la cinemática inversa, también es vital que el \textit{Asset} sea altamente eficiente y eficaz, ya que la realidad virtual tiene una gran carga de CPU y GPU. 

Por lo tanto, la realidad virtual requiere que \textit{IK} se resuelva no solo en alta frecuencia, sino también en alta calidad: todo se vuelve observable con gran detalle y en primera persona más aun, ya que incluso debe estar a la altura de la comparación con la realidad.

Teniendo en cuenta todo esto, se optó por la solución \texttt{Final IK} \footnote{\url{http://root-motion.com/}}, la cual cumplía con todos estos requisitos, incluyendo la compatibilidad con VR. 

\subsection{ARCore}
\label{cap5:sec:ARCore}

En la industria para el desarrollo de aplicaciones \texttt{AR} existe una gran competencia. Esto se debe a la creciente popularidad que grandes empresas tecnológicas, como Apple y Google, están ejerciendo con sus propios \textit{SDKs} de desarrollo de AR. Apple lanzó \texttt{ARKit} en 2017, y solo un año después, Google presentó \texttt{ARCore}.

Las aplicaciones de AR están desarrolladas para dispositivos móviles que pueden ser dispositivo iOS, Android o auriculares AR más sofisticados como Hololens \footnote{\url{ https://www.microsoft.com/es-es/hololens}} y Magic Leap \footnote{\url{ https://www.magicleap.com/en-us}} utilizados en soluciones empresariales más profesionales.

En la actualidad, existen tres \textit{SDKs} de AR integrados en Unity como \textit{Game Engines}. Se trata de ARkit, ARCore y Vuforia. 

ARKit es un conjunto de herramientas creadas por Apple para ayudar a los desarrolladores a crear aplicaciones de realidad aumentada para dispositivos iOS. Con ARKit solo se puede desarrollar aplicaciones AR para iPhones y iPads, más detalladamente desde el iPhone 6s en adelante y iPads a partir del iPad Pro.

ARKit se lanzó con IOS 11 en 2017. En ese entonces, al desarrollar aplicaciones AR, se suponía que debía usar el marco de desarrollo SceneKit de Apple, sobre el cual se construyó la primera versión de ARKit. SceneKit se lanzó en 2012 y recibió pocas actualizaciones hasta la llegada de RealityKit, la tercera versión de ARKit.

ARKit 3 viene con varias características nuevas e innovadoras, como:

\begin{itemize}
    \item \texttt{Oclusión ambiental}: el contenido 3D AR pasa de manera realista detrás y delante de las personas en el mundo real.
    \item \texttt{Seguimiento de caras}: detección de hasta tres rostros a la vez.
    \item \texttt{Captura de movimientos}: utiliza poses y gestos, que interactúan con los movimientos humanos.
\end{itemize}

\texttt{Vuforia} es una de las empresas de RA más antiguas del mercado. Después de su adquisición en 2015 por PTC Inc., \texttt{Vuforia}ha ampliado su línea de herramientas orientadas a RA. Estas herramientas ahora incluyen productos como Vuforia Engine y Vuforia Studio, que se utilizan en el desarrollo de aplicaciones de RA.

\texttt{Vuforia} puede ejecutarse tanto en iOS como en Android e incluso en los modelos más antiguos de iPhone con los que ARKit no es compatible. Además, Vuforia usa ARKit o ARCore cuando el hardware en el que se ejecuta lo admite, de lo contrario, puede utilizar su propia plataforma.

Estas serían algunas de las características más señaladas de este \textit{SDK}:

\begin{itemize}
    \item \texttt{Seguimiento de objetos}: ofrece detección de objetos , de modo que tanto las imágenes como las formas pueden actuar como marcadores.    
    \item \texttt{Reconocimiento de textos}: detección de textos, como si se tratase de objetos 3D.
    \item \texttt{VuMark}: este sistema es capaz de detectar tanto imágenes como códigos QR.
\end{itemize}

El último de los \textit{SDKs} disponibles en el mercado es \texttt{ARCore}. Es la respuesta que lanzón Google en 2018 al \texttt{ARKit} de Apple. Se trata de una plataforma para el desarrollo de aplicaciones AR en Android (7.0 o superior) e iOS (11 o superior). Además, este kit de herramientas de desarrollo de AR está disponible de forma gratuita tanto para Unity como para Unreal Engine.

\texttt{ARCore} ofrece grandes posibilidades para el desarrollo de aplicaciones AR, entre las que caben destacar tres de ellas:

\begin{itemize}
    \item \texttt{Seguimiento del movimiento}: es crucial no solo poner objetos virtuales en el mundo real, sino también asegurarse de que se vean realistas desde todos los ángulos. ARCore garantiza esto alineando la cámara virtual 3D que muestra su contenido 3D con la cámara del dispositivo.
    \item \texttt{Oclusión ambiental}: este sistema detecta planos y puntos característicos para que pueda colocar correctamente objetos virtuales en superficies planas reales. Por ejemplo, objetos en una mesa o paredes.
    \item \texttt{Estimación de la luz}: utilizando la cámara de un teléfono, ARCore puede detectar las posiciones de iluminación actuales en el mundo físico. Por lo tanto, este sistema ilumina los objetos virtuales de la misma manera que los objetos reales, lo que aumenta la sensación de realismo.
\end{itemize}

Las tres plataformas son perfectamente capaces de proporcionar las herramientas necesarias para el desarrollo de una aplicación de AR.

La decisión final radicó en utilizar un ecosistema en el cual no existiera una limitación en la plataforma y poder aprovechar la potencia del \textit{SDK}, junto con los numerosos servicios que ofrecía. Por todo ello, se eligió \texttt{ARCore} de Google, ya que brinda una mayor flexibilidad en cuanto a los términos donde desplegar la aplicación.

\section{Metodología y desarrollo de \textit{Mocap} en VR}
\label{cap5:sec:capitulo5}

Como el objetivo principal de este proyecto es la captura de movimiento para entrenamiento de actividades físicas en VR, se selecciona como estudio, \texttt{VRIK Calibration}, dado su potencial para calibrar las características personales del profesor, y el seguimiento realizado por el avatar virtual.

\subsection{Investigación base}

VRIK tiene su propio conjunto de restricciones integradas y los límites de rotación no se pueden utilizar en el proceso de resolución. Sin embargo, es posible aplicar \textit{RotationLimits} encima de VRIK, por ejemplo, para asegurarse de que los huesos de la mano no se doblen de forma poco natural más allá de los límites razonables. Para hacer esto, tendríamos que deshabilitar los límites de rotación en el inicio para tomar el control de la actualización de las posiciones del avatar y luego actualizarlos usando \textit{VRIK.solver.OnPostUpdate}

%\vspace{2cm}
\begin{lstlisting}
    public VRIK ik;
    public RotationLimit[] rotationLimits;
    void Start() {
        foreach (RotationLimit limit in rotationLimits) {
            limit.enabled = false;
        }
        ik.solver.OnPostUpdate += AfterVRIK;
    }
    private void AfterVRIK() {
        foreach (RotationLimit limit in rotationLimits) {
            limit.Apply();
        }
    }
\end{lstlisting}


Tras revisar las restricciones que se pueden utilizar en las rotaciones de los huesos de un personaje humanoide en Unity, nos adentramos en la escena de calibración de VRIK. Para este proceso, es necesario que el avatar a utilizar tenga definidos los huesos del esqueleto dentro del \textit{mesh} (clase de Unity que permite crear o modificar mallas a partir de scripts) y la relación entre cada uno de ellos. El proceso para realizar este objetivo se denomina \textit{rigging}. Si el avatar a utilizar no tuviera esta característica se podría conseguir usando el \textit{Auto-Rigger} de Mixamo, ya definido en la sección \ref{cap4:sec:mixamo}  cuando se describió el software utilizado en el proyecto.

Ya en la escena \texttt{VRIK Calibration} se observa una demostración de como usar el calibrador VRIK, de modo que se ayude a calibrar las posiciones donde se encuentran los componentes de VR que harán que el sistema de captura de movimiento tome forma.

Al adentrarnos en los componentes que resuelven el problema ocasionado por la cinemática inversa relacionado con modelos humanoides, se observa el script \texttt{VRIK} que a continuación se describen en los siguientes puntos:

\begin{itemize}
    \item \texttt{VRIK.fixTransforms}: en el caso de habilitar esta opción, esta solución arreglará todos los \textit{Transforms} (componente de Unity que se utiliza para almacenar y manipular la posición, rotación y escala de un objeto) utilizados a su estado inicial en cada \textit{Update} (descrito en la sección \ref{cap4:sec:unity}). Evitando posibles problemas ocasionados por huesos no animados. Este problema también ayudamos a solventarlo con la creación del script \texttt{VICIK} utilizando las referencias de los seis puntos de seguimiento utilizados en este proyecto.
    \item \texttt{VRIK.references}: se trata del mapeo óseo de un avatar humanoide, en el caso de que el modelo 3D a utilizar contenga las 22 referencias correspondientes a las articulaciones del cuerpo humano esta asignación se hará automáticamente, por el contrario si el avatar a utilizar no contiene estas referencias de forma ordenada, será necesario realizar esta asignación de forma manual.
    \item \texttt{VRIK.solver}: es el encargado de realizar junto con la asignación de los seis componentes que vamos a utilizar para la captura de movimiento, que todo el sistema funcione de forma fluida y estable. Pudiendo ajustar la posición y rotación del avatar virtual, para que coincida con la orientación de cada uno de los huesos del profesor.
\end{itemize}

\subsection{Desarrollo para la grabación de movimientos}

Después de haber realizado la investigación y compresión del material que se puede aprovechar para realizar una captura de movimiento, se plantearon una serie de desafíos e implementaciones que se deben realizar.

Para que un sistema \textit{Mocap} sea de calidad y dé la sensación de realismo al reproducir los movimientos grabados, es necesario como mínimo tener tres puntos de seguimiento, como por ejemplo las gafas de realidad virtual y dos controles, uno para cada mano. Este tema sobre como captura los movimientos los sensores ópticos se trato en la sección \ref{cap3:sec:realidadVirtual}.

Visualizando la escena de \texttt{VRIK Calibration} y aunque con tres puntos de seguimiento se puede hacer un \textit{Mocap} decente, se decidió utilizar una captura de movimiento con seis puntos de detección. Esta decisión se llevó a cabo ya que el arte marcial afrobrasileño (capoeira) realiza movimientos complejos y utiliza todas las partes del cuerpo, de modo que era indispensable captar toda esa información de la mejor manera posible. 
Para ello se desarrolló un script(\texttt{VICIK}) específico para este fin, el cual se describirá a continuación.

En primer lugar se creó el script \texttt{VICIK} el cual gestionará el conjunto de la captura de movimiento. Para ello fue necesario determinar un \textit{Game Object} (clase base para todas las entidades en una escena en Unity) de tipo \textit{VRIK}, el cual ya nos permitía acceder a todas las características de este objeto, y por lo tanto modificar las referencias del script \textit{VRIK} de forma ordenada (véase Figura \ref{fig:GrafoMocap}). 

Como se describió anteriormente, \texttt{VRIK.references}, contiene todas las referencias óseas del cuerpo humanoide a utilizar, pero en el caso de que el modelo 3D(avatar) que vayamos a utilizar tenga una jerarquía de huesos diferente a la utilizada en VRIK será necesario pasar un previo proceso de \textit{rigging}. Para realizar este proceso, tras crear nuestro avatar con \texttt{Adobe Fuse Character Creator}, subiremos dicho modelo al sistema de \textit{Auto-Rigger} de Mixamo.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.3]{GrafoMocap}
    \caption{Grafo sobre la creación del entorno del \textit{Mocap}}
    \label{fig:GrafoMocap}
\end{figure} 

Con el avatar rigueado correctamente, procedemos a automatizar el proceso de asignación de las referencias de los huesos del avatar en el script. El método \texttt{AutoReferences()} realizar dicha acción. Previamente se crearan las referencias con los nombres de los huesos que utiliza \texttt{VRIK.references}, para posteriormente crear otro método \texttt{GetBonesReferences()} que vaya recorriendo todas las referencias y vaya asignado los componentes creados, con los utilizados por \texttt{VRIK}. Con todo esto, cualquier avatar que contenga los huesos creados por \texttt{Adobe Fuse Character Creator} o cualquier \textit{software similar} tendrá este proceso automatizado, y no será necesario realizar una asignación manual para cada avatar.

El siguiente apartado importante para realizar la captura de movimiento, es como poder asignar los componentes de VR (gafas, controles y \textit{trackers}) a los seis puntos que nuestro avatar iba a seguir para grabar los movimientos del \textit{Mocap}.

Para ello se creo otro script denominado \texttt{VICAvatar} el cual se encarga de realizar esta acción. Sería necesario crear una serie de referencias que contuvieran la posición y rotación de los sensores que el profesor iba a tener en el cuerpo. Para desarrollar este \textit{Mocap} se decidió que los seis puntos importantes a capturar serían los siguientes:

\begin{itemize}
    \item \texttt{Head}: como vamos a utilizar gafas de VR, este será el objeto principal del \textit{Mocap}.
    \item \texttt{Left Hand}: para las manos utilizaremos los controles, necesarios a su vez para movernos por diferentes zonas del escenario(\textit{teleporting}).
    \item \texttt{RightHand}: al igual que en la mano izquierda, se utilizará otro de los controles para seguir esta mano.
    \item \texttt{Pelvis}: en este caso utilizaremos el primero de los \textit{trackers}, utilizando un cinturón para ello adherido a nuestra cintura.
    \item \texttt{Left Foot}: para la parte de los pies, al igual que para la cintura, destinaremos otro de los \textit{trackers} sujeto al empeine.
    \item \texttt{Right Foot}: al igual que en el pie izquierdo, destinaremos uno de los \textit{trackers} para seguir sus movimientos.
\end{itemize}

Para crear el entorno de VR, es necesario la utilización de uno de los componentes de \texttt{SteamVR}, la clase \texttt{Player} vista en la sección \ref{cap5:sec:SteamVR}. Este objeto actúa como un \texttt{Singleton} (véase la Figura\ref{fig:Singleton}), lo que significa que solo debe existir un objeto \texttt{Player} en la escena. Además del objeto \texttt{Player} (componente head de nuestro sistema) que es principalmente la cámara que reproducirá las imágenes en sus dos pantalla, a modo de ojos humanos. Es necesario el uso de otro objeto para cada uno de los controles y los tres \textit{trackers} restantes. Este objeto contiene el script \texttt{Steam VR\_Behaviour\_Pose} el cual se encarga de realizar el seguimiento de un determinado componente óptico. Como previamente ya hemos creado las referencias a cada uno de nuestros huesos (leftHand, rightHand, pelvis, leftFoot y rightFoot), solo faltaría indicarle a cada objeto que tipo de hueso queremos usar.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.6]{Singleton}
    \caption{Patrón \textit{Singleton} en Unity}
    \label{fig:Singleton} 
\end{figure} 

Con las referencias de los seis huesos creados, ahora lo que tocaría hacer es asignar el seguimiento de los sensores a cada hueso. Para ello es necesario crear un objeto de tipo \textit{VICIK} y un método denominado \textit{UpdateBoneReferences()} que realice la asignación para realizar la grabación de la captura de movimiento.

Con todo el escenario ya preparado para realizar la grabación del \textit{Mocap}, se creó la escena principal del proyecto, donde estarán todos estos componentes y scripts mencionados anteriormente. 

Para realizar la secuencia de grabación, se creo un panel en VR que permitiese grabar los movimientos a modo de estudio de grabación de música, con los tres botones clásicos (play, pause y stop), además de incluirse un panel para poder escribir el nombre del movimiento que se va a realizar.

El profesor podrá visualizar el movimiento que está realizado a cada instante, ya que verá una imagen de un avatar idéntico a él a modo de espejo, de este modo podrá revisar cada una de las poses que esté realizando mientras graba los movimientos que luego visualizará el alumno en la aplicación de AR. 

Cuando el profesor esté grabando un movimiento, podrá repetirlo las veces que consideré oportunas, finalizando la grabación con el botón de stop y volviendo a pulsar el botón de \textit{record} para iniciar de nuevo el proceso. También existe la opción de pausar una grabación, y retomarla instantes después antes de finalizar la grabación, pulsando el botón correspondiente (\textit{pause}). Cuando un movimiento se quiera reproducir antes de finalizar la grabación, existirá la opción de pulsar el botón de \textit{play}, siempre y cuando se haya pulsado el botón de \textit{stop} previamente. Tras toda esta secuencia, se habilitará un botón de finalizar para concluir la grabación del \textit{Mocap} y de nuevo se podría iniciar el proceso de grabación con un nuevo movimiento.

Los movimientos grabados serán guardados como animaciones de Unity, de modo que el profesor podrá visualizar los movimientos que haya grabado hasta el momento, para en el caso deseado, poderlo repetir y borrar el movimiento capturado previamente. Además, estos movimientos capturados serán los que se podrán analizar en el otro escenario del proyecto, el entorno del alumno en realidad aumentada.

\section{Análisis de \textit{Mocap} en AR}

Otra parte importante de este proyecto consiste en poder analizar los movimientos previamente grabados por el profesor, dependiendo del nivel del alumno. De esta manera se podrá realizar una corrección del movimiento que necesita practicar el alumno en tiempo real. En esta sección se explica cómo se ha desarrollado esta parte del proyecto con realidad aumentada.

\subsection{Investigación base}
\label{cap5:sec:panteamientoOpenCV} 

El planteamiento inicial sobre el análisis de los movimientos grabados por el profesor fue muy distinto al que finalmente se desarrolló. 

En la mayoría de las artes marciales los movimientos son repetidos miles de veces en cada sesión, es por esto clave optimizar el rendimiento para evitar lesiones y obtener unos resultados eficientes en su ejecución.

La evaluación de los movimientos de capoeira puede ser realizada por el ojo humano de un entrenador, con video análisis o con equipos de biomecánica especializados, como sensores de presión o programas tridimensionales que analicen los movimientos realizados. 

En primera instancia se estudió como realizar el análisis de los movimientos capturados por el profesor mediante \textit{Deep Learning} con\texttt{OpenCV}. Este sistema de visión artificial (descrito en la sección \ref{cap3:sec:OpenCV}) utiliza una serie de algoritmos basados en modelos (Ejemplos de modelos preentrenados en ImageNet: \textit{Xception, VGG19, ResNet50}) capaces de seguir la estructura ósea del movimiento del cuerpo humano. Este método se base en representar las partes del cuerpo humano en segmentos y unirlos mediante puntos, identificando principalmente el torso, la cabeza y las extremidades.

Visualizando la gran cantidad de estudios relacionados con \textit{OpenCV} y el análisis de movimientos, no dejaría de ser un estudio más sobre como analizar y aprender este tipo de arte marcial. Dado que la tecnología a evolucionado en gran medida en estos últimos años, se opto por utilizar una herramienta novedosa y en plena evolución, como es la realidad aumentada.

Con el planteamiento de utilizar \texttt{AR} como base para el análisis de los movimientos del profesor y el posterior aprendizaje del alumno, se optó por desarrollar esta parte del proyecto con \texttt{ARCore} (ya se describieron los detalles de esta elección en la sección \ref{cap5:sec:ARCore}).

De todo el contenido incluido en el paquete de \texttt{ARCore} para Unity, se selecciona como estudio, \texttt{HelloAR}, dado el gran potencial que ha incluido \texttt{ARCore} en este escenario, un nuevo sistema de oclusión ambiental denominado \textit{Depth API}\footnote{\url{https://developers.google.com/ar/develop/unity/depth/overview}}. Este sistema utiliza la cámara RGB de los dispositivos móviles para crear mapas de profundidad, para posteriormente utilizar esta información y hacer que los objetos virtuales aparezcan con precisión 

Para entender un poco mejor como utiliza \texttt{ARCore} el sistema de profundidad vamos a explicar cómo realiza este proceso este \textit{SDK} de realidad aumentada (véase Figura \ref{fig:depth-values-diagram}). En la geometría del mundo real observamos un punto \texttt{A}, y un punto \texttt{a} 2D que representa el mismo punto pero en la imagen con el mapa de profundidad. El valor dado por esta \textit{API} es igual a la longitud de \texttt{CA} proyectada sobre el eje principal. Al trabajar con este sistema es importante destacar que los valores del mapa de profundidad no son la longitud del rayo \texttt{CA}, sino la proyección del mismo.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.2]{depth-values-diagram}
    \caption{Proyección en el plano utilizada en \textit{Depth API}}
    \label{fig:depth-values-diagram}
\end{figure} 

Ya en la escena \texttt{HelloAR} se visualiza como la API necesita tener un mapa del entorno a utilizar para poder colocar los objetos virtuales en el mundo real, permitiendo que \texttt{ARCore} establezca un seguimiento completo, detectando la geometría de la superficie con la que se quiere interactuar. Este proceso lo realiza el script \texttt{DetectedPlaneVisualizer} necesario en la utilización de este \textit{SDK}.

\subsection{Desarrollo del análisis de movimientos}

Después de haber realizado la investigación y compresión del material necesario para utilizar \texttt{ARCore} como sistema de realidad aumentada, se procede a solventar una serie de problemas o necesidades surgidas durante la investigación.

Para este apartado se observó el desarrollo realizado en VR sobre la grabación de movimiento explicada anteriormente. En esta sección el alumno podrá tener un\textit{feedback} visual inmediato y novedoso, de como poder realizar un movimiento de forma correcta en el arte marcial brasileño (capoeira).

Con el fin de crear un entorno de AR, es necesario la utilización de uno de los \textit{prefabs} de \texttt{ARCore}, \texttt{ARCoreDevices} el cual incluye la configuración de la sesión a utilizar en realidad aumentada (\textit{SesionConfig y CameraConfigFilter}). En este escenario donde vamos a desarrollar una aplicación para el análisis visual de los movimientos grabados por el profesor en VR es necesario establecer unas configuraciones avanzadas detalladas a continuación:

\begin{itemize}
    \item \texttt{Config.PlaneFindingMode}: selecciona el comportamiento del sistema de superficie a detectar. En este desarrollo utilizaremos un sistema que solo detecte planos horizontales.
    \item \texttt{Config.CloudAnchorMode}: define una ubicación específica rastreada en el mundo real. De modo que guarda la posición real de objetos 3D para luego ser utilizados. En este desarrollo no utilizaremos esta opción, ya que no queremos tener un punto fijo para visualizar los movimientos del profesor.
    \item \texttt{Config.FocusMode}: tipo de enfoque de la cámara a utilizar, en los dispositivos compatibles como el utilizado para el desarrollo del proyecto, se utilizará el tipo \textit{Fixed} para optimizar el seguimiento en AR.
    \item \texttt{Config.DepthMode}: modo del mapa de profundidad a utilizar. En los dispositivos compatibles, la mejor profundidad posible se estima en función del hardware y software del dispositivo. Proporciona una estimación de profundidad para cada píxel de la imagen. El inconveniente es que agrega una carga computacional significativa. Para el desarrollo de este proyecto utilizaremos el modo \textit{Automatic} para que pueda ser utilizado en diferentes dispositivos.
\end{itemize}

Como ocurría en el escenario de VR, es necesario la utilización de un componente que actúa como un \texttt{Singleton} (véase la Figura\ref{fig:Singleton}), de modo que solo puede existir un objeto de tipo \texttt{FirstPersonCamera} en la escena. Este objeto como su propio nombre indica, se trata de la cámara que utiliza el sistema para mostrar toda la información desarrollada en realidad aumentada. Este componente estará incluido en el script que se describe a continuación.

Otro de los scripts necesarios es \texttt{ARCoreSesion}, el cual determina cual de las dos cámaras (frontal o trasera) se utilizará en la aplicación a desarrollar. Además serán necesarios configurar dos archivos denominados \textit{SesionConfig y CameraConfigFilter}. Estos archivos con extensión \textit{.asset de Unity} se utilizan para detallar la configuración de la cámara que \texttt{ARCore} utiliza para acceder al sensor de la cámara para una sesión determinada. Estos detalles incluyen, por ejemplo, la velocidad de fotogramas que captura el objetivo y si la cámara a utiliza un sensor de profundidad.

Al crear una nueva sesión de \texttt{ARCore}, este utiliza un filtro para determinar la configuración de la cámara que mejor coincida con la lista de configuraciones disponibles. En el desarrollo de esta aplicación se utilizó el recurso \textit{CameraConfigFilter} para reducir las configuraciones de cámaras disponibles en una serie de dispositivos en tiempo de ejecución, según las necesidades de dadas.

Para la creación de esta escena, existe un script denominado \textit{HelloARController} el cual controla la instancia de la cámara a utilizar y como colocar los objetos 3D en el mundo real, pudiendo utilizar un plano vertical, uno horizontal o un punto fijo en el entorno. El problema radica en que este script instancia objetos 3D tantas veces como toques la pantalla, por lo tanto desarrollaremos un script propio denominado \texttt{ ARController} el cual se encargará de realizar una sola instancia del modelo 3D del profesor y otro script denominado \texttt{MovementManager} que gestionará de forma automática todos los movimientos grabados previamente en VR para ser asignados a la única instancia de la escena.

Con el objetivo de tener un mayor control de la aplicación de AR, se desarrolló el script \texttt{ARController}. Este script controla los siguientes objetos de Unity necesarios para dar forma a esta aplicación:

\begin{itemize}
    \item \texttt{Objecto de tipo \textit{DepthMenu}}: tipo de Mapa de profundidad que se utilizará, se podrá activar o desactivar en todo momento en tiempo de ejecución (para que los dispositivos no compatibles con este sistema puedan utilizar la aplicación).
    \item \texttt{Objecto de tipo \textit{HorizontalPlanePrefab}}: se utilizará una instancia única cuando el alumno pulse la pantalla, siendo solamente utilizada para una superficie a detectar de tipo horizontal.
    \item \texttt{Objecto de tipo \textit{GameObject}}: este \texttt{Prefab} utilizará el método \texttt{Update()} de Unity (descrito en la sección \ref{cap4:sec:unity}) para detectar en cada \textit{frame} el instante preciso para crear el avatar del profesor, la posición, rotación y escala que tiene en cada momento dicho modelo 3D.
\end{itemize}

Para controlar toda la información obtenida en la grabación de movimientos en VR, se creó el script \texttt{MovementManager} capaz de gestionar la concurrencia existente entre los diferentes movimientos capturados por el \textit{Mocap}. En primer lugar, se creará una instancia única del avatar que utilizará el profesor del tipo \textit{Animator}, de este modo el siguiente paso a realizar será la asignación de forma automática de todos los movimientos grabados previamente. Esta asignación se realizará mediante una lista de tipo \textit{Animator} denominada \textit{movements} (List<Animator> movements). Para poder crear una asignación automática de los movimientos, es necesario crear un objeto \textit{AnimatorController} y un componente de Unity del mismo tipo. 

Teniendo todo esto, iniciamos el proceso de automatizar la asignación de los movimientos ya incluidos en la lista (ya que han sido importados en el proyecto, en una carpeta específica para tal fin). Para ello se creó un método denominado \texttt{AddMovementAnimator()} que recorre la lista y va asignado a modo de diagrama de estados, cada uno de los movimientos, con otro de ellos. De este modo, todos los movimientos podrán ser seleccionados una y otra vez en la interfaz de AR. 

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.3]{AnimatorController}
    \caption{Asignación automatizada de los movimientos grabados en el \textit{Mocap}}
    \label{fig:AnimatorController}
\end{figure} 

Además de todo este proceso, se creó un método el cual creará los botones de la interfaz en tiempo de ejecución denominado \texttt{MovementChanges()}, de este modo el método recorrerá la lista de movimientos guardados en la lista \textit{movements} y creará tantos botones en la \texttt{UI} como movimientos grabados existan, siempre dependiendo del nivel seleccionado previamente por el alumno.

En el planteamiento de como el alumno analizaría los movimientos grabados por el profesor para su posterior aprendizaje, se decidió que esta parte del proyecto se iba a desarrollar en realidad aumentada (como se explicó en la sección \ref{cap5:sec:panteamientoOpenCV}) debido a su gran potencial. 

Para este fin, se utilizó un sistema capaz de reproducir, pausar, aumentar y disminuir la velocidad del movimiento realizado por el profesor, para así poder analizar en 360º cada una de las poses que realiza el profesor para ejecutar la transición en cada movimiento del arte marcial brasileño (capoeira).(véase Figura \ref{fig:GrafoAR}) El alumno podrá visualizar diferentes movimientos, dependiendo del nivel elegido al iniciar la aplicación, de modo que el profesor previamente ha grabado movimientos para todos los niveles seleccionables (principiante e intermedio). Todo este sistema ha sido desarrollado en un script denominado \textit{UIController}, creando instancias para cada uno de los botones seleccionables a la hora de reproducir los movimientos, además de los botones de reproducción, pause y el \textit{slider} capaz de aumentar el ritmo que utilizar el profesor en el movimiento capturado en VR.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.3]{GrafoAR}
    \caption{Análisis de lo los movimientos grabados en el \textit{Mocap}}
    \label{fig:GrafoAR}
\end{figure} 

Otro de los sistemas importantes a desarrollar para dar un efecto más realista es la utilización de la estimación de luz en AR, por defecto viene deshabilitada. Para este tipo de sistemas se pueden utilizar dos tipos de modos de luz ambiental, definidos a continuación:

\begin{itemize}
    \item \texttt{HDR Environmental}: el modo de estimación de luz se establece con o sin reflejos, de modo que el componente de luz ambiental en la escena actualizará la rotación y el color de la luz, además de modificar los componentes de Unity \textit{ambient probe} mediante la propiedad \texttt{RenderSettings.ambientProbe} y \textit{reflection probe} mediante la propiedad \texttt{RenderSettings.customReflection}.
    \item \texttt{Ambient Intensity}: en este modo de estimación de luz, el componente de luz en la escena de Unity fijará \textit{\_GlobalLightEstimation} propiedad que se utiliza en la propiedad de\texttt{ARCore}  \texttt{DiffuseWithLightEstimation} y \texttt{SpecularWithLightEstimation} y otros \textit{shaders} personalizados para ajustar la salida de color final para que coincida con el color de la imagen de la cámara.
\end{itemize}

En el script denominado \texttt{ARController} se asigna mediante el método \texttt{SetLightEstimationMode()} el tipo de estimación de luz a utilizar en el proyecto de AR, pasandole al método de tipo \texttt{LightEstimationMode} el parámetro \texttt{EnvironmentalHDRwithReflections}.


\section{Grabación de los movimientos con expertos}

En esta etapa del proyecto, estaba prevista la grabación de todos los movimientos con gente experta en la práctica del arte marcial brasileño (capoeira), pero dados los impedimentos ocasionados por la \textit{COVID-19}, no se pudo completar el conjunto de los movimientos deseados, por lo tanto, hubo que utilizar algunas grabaciones de otro proyecto desarrollado en años anteriores \cite{VIC} y adaptar los datos obtenidos al uso de este tipo de \textit{Mocap}.

\subsection{Proceso de grabación de los movimientos}

\begin{figure}[h!]
    \centering 
    \includegraphics[scale=0.25]{GrafoVR}
    \caption{Transición de la grabación de movimientos mediante \textit{Mocap}}
    \label{fig:GrafoVR}
\end{figure} 

Para la realización de este proceso, fue necesario grabar una batería de movimientos del arte marcial brasileño (capoeira) y seleccionar los que fueron captados de una forma más precisa y eficiente. Para facilitar la grabación del profesor, y tener un espectro de movimientos superior, se optó por capturar varias repeticiones del mismo movimiento. 

Dado que los componentes de captura de movimiento utilizados permiten emular la presencia de auriculares y controladores de realidad virtual, es necesario que el profesor se habitué a la utilización de este tipo de sistemas, para que el proceso de grabación se realice de una forma más eficiente.

El proceso desarrollado para la grabación de los movimientos se diseñó siguiendo una máquina de estados capaz de alternar entre la metodología elegida para la captura de movimientos en el arte marcial brasileño(capoeira) y la parte analítica y visual de los movimientos grabados por el profesor. 

\begin{figure}[h!]
    \centering 
    \includegraphics[scale=0.4]{Datos}
    \caption{Datos capturados en otro sistema de \textit{Mocap}}
    \label{fig:Datos}
\end{figure}

Inicialmente para grabar los movimientos del \textit{Mocap} desarrollado, es necesario equiparse de seis dispositivos (gafas de \textit{VR}, dos controladores, uno para cada mano, un \textit{tracker} situado en la cadera, y dos \textit{trackers} más situados en cada uno de los pies). Tras el equipamiento, el profesor se dirige al entorno donde se grabarán los movimientos. En este apartado (véase Figura \ref{fig:GrafoVR}), el profesor se verá a si mismo en un espejo, de este modo podrá comprobar de una manera muy precisa cual está siendo el resultado de la grabación. En todo momento, mientras se encuentra grabando podrá pausar la grabación, reanudarla en el punto exacto donde la dejó o detenerla si el resultado no es el deseado. Tras finalizar la grabación, el sistema guardará la transición de los movimientos en una base de datos compartida entre la aplicación de \textit{VR} y \textit{AR}. De este modo, en cuanto el movimientos ha sido grabado, tanto el profesor podrá revisarlo para ver su resultado, como el alumno podrá visualizarlo en su \textit{Smartphone} mediante la aplicación de AR.

Otro de los puntos importante es el análisis de los movimientos previamente grabados. El usuario podrá visualizar todos los movimientos que se encuentran en la base de datos, de modo que podrá revisar las veces que sea necesario, la transición de cada uno de los huesos del cuerpo ejecutando cada una de las poses. En el caso de que el movimiento grabado no haya quedado preciso, el profesor podrá borra dicho movimiento y trasladarse a la zona de grabación para crearlo de nuevo.

Este sistema se creó principalmente para que el profesor pudiera grabar los movimientos que el alumno debería practicar, pero igualmente este sistema dispone de un modo adicional al análisis tratado en \textit{AR}. El alumno podría igualmente practicar los movimientos que el profesor le habría determinado en \textit{VR} revisando lo movimientos que está realizando mirando la transición de sus poses en el espejo.

Dados los problemas ocasionados por la \textit{COVID-19}, no fue posible realizar de una forma completa la grabación de los 16 movimientos, pudiendo solo terminar 12 de ellos antes del inicio de la pandemia. Para solventar este problema, dado que en años anteriores se desarrollo otro proyecto donde se habían capturado movimientos de expertos en capoeira, se optó por analizar esos datos y prepararlos para su utilización en este proyecto.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale=0.4]{Animator}
    \caption{Información detallada de la transformación de los datos grabados}
    \label{fig:Animator}
\end{figure} 

Estas grabaciones utilizaban un sistema completamente diferente al desarrollado en este proyecto, ya que todo el contenido de los huesos del avatar capturado era guardado en ficheros \textit{.txt}(véase Figura \ref{fig:Datos}) con toda la información de las posiciones y rotaciones, en cada una de las transiciones realizadas.

Para ajustar estos datos, fue necesaria la utilización de un componente de \textit{Unity} denominado \textit{Animator} el cual puede ayudar a transformar la infinidad de datos de los 25 huesos del cuerpo que utilizaba este anterior sistema. Tuvo que realizarse un ajuste manual para cada frame, ya que los datos capturado por este sistema utilizaban \textit{kinect}, siendo este un sistema mucho menos preciso que el utilizado en este proyecto. Para realizar el ajuste en el conjunto de todos los huesos, fue necesario separa las 25 transiciones de cada hueso para así ir ajustando \textit{frame} a \textit{frame} cada uno de los movimientos (véase Figura \ref{fig:Animator}), determinado las posiciones y rotaciones existentes y ajustando estos valores para que los movimientos fueran lo más precisos posible.

Con todo ello, y añadiendo la supervisión del profesor en los movimientos que fue posible, se determinaron 16 acciones para el uso de este proyecto, detallados a continuación:

\begin{itemize}
	\item \textit{Ginga}, es la técnica fundamental de la capoeira, consiste en realizar un movimiento constante que prepara al usuario para acciones como evadir, fintar o atacar, así como conservar el impulso. La forma general de la ginga es un continuo paso triangular, retrocediendo un pie y luego con el otro en diagonal mientras se mantiene las piernas separadas. Para este movimiento se realizaron tres grabaciones diferentes porque es un movimiento que puede tener diversas variantes.
	\item \textit{Armada}, técnica en donde el usuario gira sobre si mismo en vertical golpeando con el talón al oponente.
    \item \textit{Bençao}, técnica que se ejecuta con una patada frontal básica, realizada contra el abdomen o el pecho del oponente.
	\item \textit{Queshada}, movimiento que consiste en dar un paso quedándose de costado al oponente y se realiza una patada con un movimiento circular.
	\item \textit{Chapa}, movimiento similar al Bençao pero se realiza con la planta del pie y paralelo al suelo.
	\item \textit{Gancho}, técnica que consiste en levantar la pierna en un diagonal hasta una posición alta, para después contraer la rodilla  y golpear con el talón en dirección descendente.
	\item \textit{Martelo}, movimiento de patada que tiene como objetivo golpear con el empeine en la sien del oponente.
	\item \textit{Meia Lua de Compasso}, técnica que consiste en apoyar una mano en el suelo mientras se gira 180º y se lanza la pierna opuesta en un movimiento circular y ascendente, para golpear con el talón en la cabeza del oponente.  
	\item \textit{Meia Lua de Frente}, movimiento que realiza una patada frontal de fuera a dentro
	\item \textit{Ponteira}, movimiento que consiste en estirar la pierna hacia el oponente y golpearlo con la punta del pie.
	\item \textit{Joelhada}, movimiento que se realiza alzando la rodilla hacia delante con la intención de golpear.
	\item \textit{Galopante}, técnica que realiza un golpe con la mano abierta parecido a una bofetada.
	\item \textit{Telefone}, técnica que sirve para golpear con las dos manos en los tímpanos del oponente.
	\item \textit{Desprexo}, movimiento que consiste en dar una bofetada pero con la parte externa de la mano.
	\item \textit{Skada}, movimiento que consisten en lanzar golpes consecutivos hacia delante con la mano abierta.
	\item \textit{Esquiva de frente}, es un técnica para esquivar un ataque elevado y consiste en desplazar el cuerpo hacia abajo.
\end{itemize}

Después del proceso de grabación, se determinó que de los 16 movimientos grabados que mejor que se adaptaban a esta captura de movimientos eran: Armada, Bencao, Chapa, Esquiva, Martelo, Meia Lua De Compasso, Meia Lua De Frente y Queshada.

Con estos 8 movimientos se realizó un gran trabajo de ajuste, quedando un resultado muy satisfactorio dado que se utilizó un sistema con 6 puntos de referencia para la captura de movimiento.


\section{Diseño y creación de avatares}

En esta etapa del proyecto, con el objetivo de dar una sensación más realista al \textit{Mocap}, se han creado una serie de características estéticas en los avatares, siendo esta, similar a los profesionales que practican capoeira. De modo que la experiencia a proporcionar sea más amena para el usuario.

\subsection{Investigación base} 

En la búsqueda de software dedicado al modelado de personajes 3D se encontraron aplicaciones como \texttt{Blender}\footnote{\url{https://www.blender.org}}, \texttt{3ds Max}\footnote{\url{https://www.autodesk.es/products/3ds-max}}, \texttt{ZBrush}\footnote{\url{https://pixologic.com/}}, entre otras. 

Aunque la idea de esculpir en un escenario 3D puede sonar atractiva, ya que estos sistemas son capaces de crear objetos paramétricos y orgánicos con características de polígono, superficie de subdivisión y modelado basado en \textit{spline} (funciones utilizadas en aplicaciones de modelados 3D que requieren la interpolación de datos, o un suavizado de curvas). Las características interesantes (para los diseñadores en particular) son las herramientas de modelado basadas en NURBS (modelo matemático muy utilizado en programas de modelado 3D para generar y representar curvas y superficies), ya que en estos programas de diseño 3D permiten mallas orgánicas y matemáticamente precisas. Entre las otras técnicas está la capacidad de crear modelos a partir de datos de nube de puntos.

Este tipo de sistemas no son de ninguna manera programas de diseño 3D que puedas dominar intuitivamente. Tienen una curva de aprendizaje empinada, se necesitan muchas horas de práctica para dominar sus muchos pinceles y herramientas, sólo entonces es cuando se producen resultados satisfactorios. De modo que tienen una curva de aprendizaje demasiado larga para las necesidades dadas. 

También se examinó una aplicación llamada \texttt{MarvelousDesigner}\footnote{\url{https://www.marvelousdesigner.com/}} la cual se emplea para desarrollar prendas de vestir. El problema surgía cuando se intentaba importar el avatar creado con \texttt{Fuse Character Creator }, ya que no eran compatibles los formatos de las dos aplicaciones y por lo tanto se descartó seguir por esta vía. 

En el camino se observó que existía una aplicación para el desarrollo de avatares compatibles con una solución de captura de movimiento y a su vez, un entorno dedicado a la conexión entre la creación de modelos 3D y el ajuste de rigueado que debe tener un avatar para este proyecto. Por este motivo, se tomó la decisión de utilizar \texttt{Adobe Fuse Character Creator}\footnote{\url{https://www.adobe.com/es/products/fuse.html}} como aplicación para el desarrollo de los personajes, ya que se amoldaba perfectamente a las necesidades de este proyecto.

\subsection{Diseño de los avatares}

Con el objetivo de crear un entorno más realista, se crearon dos avatares diferentes, cada uno con una estética propia. A la hora de elaborar el vestuario de los personajes, se observó la vestimenta utilizada por los integrantes de escuelas que practican este arte marcial brasileño. Se trata de un pantalón largo y una camiseta o sudadera en color blanco. 

Con el propósito de crear una ropa muy similar a la utilizada en capoeira, se usaron prendas prediseñadas en la aplicación \texttt{Adobe Fuse Character Creator} como los pantalones, camisetas y sudaderas que utilizarían los avatares del sistema con los retoques apropiados para darle ese color blanco característico de este arte marcial.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.6]{Rigger}
    \caption{Auto-Rig con 65 huesos de Mixamo}
    \label{fig:Rigger}
\end{figure} 

Tras completar la estética de los personajes, \texttt{Adobe Fuse Character Creator} ofrece la posibilidad de configurar los huesos de los avatares para posteriormente iniciar el proceso de rigueado que suministra \texttt{Mixamo}. La misma aplicación \texttt{Adobe Fuse Character Creator} proporciona el servicio de conexión con \texttt{Mixamo}, de modo que solo sería necesario seleccionar la opción de la aplicación para subir los avatares creados a los servidores de \texttt{Mixamo}. Cuando se complete este proceso, la aplicación redirigirá su actividad al navegador web. 

\texttt{Mixamo} también ofrece la posibilidad de crear varios esqueletos para un mismo avatar, en este caso se presentan cuatro posibles escenarios dependiendo de las necesidades dadas con 25, 41, 49 y 65 huesos respectivamente (véase Figura \ref{fig:Rigger}). La diferencia mayoritariamente depende de los huesos que queremos que tenga el avatar en las manos.

Una vez investigada la generación y construcción de los huesos de los avatares. El avatar elegido que mejor se adapta a este \texttt{Mocap} es el que contiene 65 huesos, ya que demuestra una movilidad correcta y en el futuro podrá ser usado en sistemas de captura de movimiento incluyendo \textit{hand tracking} (véase Figura \ref{fig:HandTracking}) (sistema por el cual, una serie de cámara colocadas en las gafas de VR o AR detectan cualquier movimiento realizado con las manos).

\begin{figure}[h!]
    \centering
    \animategraphics[loop,autoplay,width=0.8\linewidth]{10}{/HandTracking/HandTracking-}{0}{80}
    \caption{\textit{Hand tracking} en Oculus Quest}
    \label{fig:HandTracking}  
\end{figure}


\section{Desarrollo y Diseño de los escenarios}

En el \textit{Game Engine} utilizado, todo el contenido del proyecto está englobado en escenas. Estas se pueden utilizar para crear todo tipo de \textit{UIs}, mensajes, transición entre diferentes niveles y un sinfín de posibilidades. Para el desarrollo de este proyecto se ha diferenciado el uso de un entorno en realidad virtual y otro en realidad aumentada

\subsection{Entorno de realidad virtual}

Se trata de la escena principal del proyecto. Presenta una interfaz de tipo \textit{VR}, tras colocarse de forma correcta todos los dispositivos adheridos al cuerpo, la aplicación se encuentra inicialmente esperando a que el usuario pueda trasladarse a una de las dos ubicaciones de este entorno. (véase Figura \ref{fig:Escena1}). 

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.4]{Escena1}
    \caption{Escena principal de la aplicación de \textit{VR}}
    \label{fig:Escena1}
\end{figure} 

Primero se visualiza el punto donde se efectuarán las grabaciones de los movimientos, siendo estos efectuados delante de un espejo (véase Figura \ref{fig:Escena2}). Para la realización de las transiciones existentes en una grabación, se diseñaron una serie de botones, capaces de interactuar con el usuario mediante respuestas visuales(cambiando la visualización de los botones dependiendo del estado en el que se encuentre), como respuestas apticas, ya que los controles de \textit{VR} permiten la vibración del dispositivo, una vez que este choca contra un objeto, en este caso los botones de grabación.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.4]{Escena2}
    \caption{Escena de la grabación de los movimientos en \textit{VR}}
    \label{fig:Escena2}
\end{figure} 

Tras finalizar la grabación de los movimientos, nos teletransportamos al lugar donde se visualizarán los movimientos grabados por un avatar simulando ser el profesor de la aplicación (véase Figura \ref{fig:Escena3}). Al igual que ocurre con los otros botones anteriormente mencionados, estos también disponen de respuestas apticas y visuales, siendo capaces de dar una sensación de inmersión muy superior a la utilización de sistemas 2D.

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.4]{Escena3}
    \caption{Escena para la reproducción de los movimientos en \textit{VR}}
    \label{fig:Escena3}
\end{figure} 

Como se puede visualizar en cada una de las partes, se ha decidido utilizar un gimnasio como referentes para la práctica de la capoeira, ya que junto con el sistema de \textit{VR} se consigue tener un entorno amigable y familiar para desempeñar un entrenamiento de forma exitosa.

\subsection{Entorno de realidad aumentada}

Considerando que, al comenzar a formarse en un arte marcial, se desconocen los nombres de los movimientos que se desean poner en práctica, se le ofrece al alumno la posibilidad de visualizar previamente los movimientos antes de empezar a practicar, de modo que, podrá seleccionar más fácilmente el que desea.

Este entorno, está pensado para ser utilizado por el alumno, de modo que pueda aprender a realizar diferentes movimientos de capoeira, sin la necesidad de que esté presente un profesor. El entrenamiento consiste en visualizar de una forma más efectiva cada una de las transiciones que realiza el profesor para completar un movimiento de forma exitosa, los cuales han sido previamente capturados por expertos en el arte marcial brasileño. El alumno podrá visualizar de una manera más lenta los movimientos, de modo que podrá fijarse mejor en los errores que puede estar cometiendo al entrenar. Con la ayuda del \textit{Smartphone} el alumno podrá practicar y aprender capoeira en cualquier entorno, sin la necesidad de tener que estar en una ubicación determinada o practicarlo a una hora fija. 

\begin{figure}[h!]
    \centering 
    \includegraphics[scale = 0.2]{Escena4}
    \caption{Escena para el análisis de los movimientos en \textit{AR}}
    \label{fig:Escena4}
\end{figure} 

Como se puede observar (véase Figura \ref{fig:Escena4}) este sistema utilizar realidad aumentada para reproducir los movimientos del profesor, utilizando a su vez una interfaz de tipo \textit{UI} adaptada al sistema de \textit{AR}. Los movimientos que el alumno puede practicar se corresponden al nivel que el profesor ha determinado que debe seguir el alumno, siendo este principiante o intermedio, de este modo, el alumno va aprendiendo a realizar los movimientos de forma progresiva.

Posteriormente a la selección del movimiento, el alumno deberá analizar las transiciones que está realizando el avatar del profesor, pudiendo pausar en cualquier momento el transcurso del entrenamiento, así como, detener la visualización de una determinada transición, pudiendo elegir otro que el alumno seleccione de la lista de movimientos grabados previamente por el profesor. 


Gracias a que el sistema capta cada una de las transiciones exactas que el profesor realiza cuando graba los movimientos, el aprendiz podrá realizar un análisis preciso \textit{frame a frame} de las posiciones en las que está cometiendo algún error en la ejecución del entrenamiento.

\section{Conclusiones sobre el desarrollo del proyecto}

A lo largo de este capítulo, se han logrado explicar los estudios y desarrollos necesarios para este proyecto. La primera decisión importante fue elegir el sistema a utilizar para la captura de movimiento, siendo este, un sistema de \texttt{realidad virtual} capaz de capturar cada una de las transiciones necesarias para disponer de un sistema de calidad. 

La utilización de uno de los paquete de \textit{Unity} denominado \textit{FinalIK} fue un pilar muy importante, para ayudar a resolver el problema existente sobre la cinemática inversa en cuerpos humanoides.

Los datos obtenidos de los sensores de \textit{VR} fueron interpretarlos y mostrarlos a través de los diferentes movimientos de los avatares. Gracias a ello, se consiguieron guardar y catalogar los movimientos para realizar un análisis más preciso de las transiciones. El funcionamiento de videojuegos \textit{VR} fue una inspiración para el desarrollo de la grabación de movimientos.

Posteriormente, se diseñaron avatares con el programa \texttt{Fuse Character Creator}, para dar una apariencia más realista a las aplicaciones. Con el objetivo de dar un toque más dinámico, se diseñaron dos entornos completamente diferentes en pleno auge, como son la realidad virtual y la realidad aumentada.

El primero, simula un gimnasio dando la sensación al usuario de que está entrenando (véase Figura \ref{fig:Escena1}). El segundo escenario, simula un entorno completamente abierto, ya que los objetos se superpondrán sobre el mundo real. Además, en el segundo escenario se mostraron los movimientos que el usuario debería realizar.

Con el objetivo de completar la comparación de los movimientos, se logró desarrollar un proceso que realiza un análisis de cada parte del cuerpo. De esta manera, se contrasta con el movimiento a realizar, mostrando el entrenamiento como si se tratase de la realidad.

Y por último, se consiguieron grabar movimientos de gente experta en capoeira con el fin de catalogarlos y prepararlos para incorporarlos al proyecto.

